{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(path):\n",
    "    for root, folder, files in os.walk(path):\n",
    "        for filename in files:\n",
    "            newPath = root +\"/\"+filename\n",
    "            file = open(newPath, encoding='latin1')\n",
    "            lines = file.readlines()\n",
    "            content = []\n",
    "            inBody = False\n",
    "            for line in lines:\n",
    "                if line == '\\n':\n",
    "                    inBody = True\n",
    "                if inBody:\n",
    "                    content.append(line)\n",
    "            file.close()\n",
    "            msg = \"\\n\".join(content)\n",
    "            yield msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(path,classification):\n",
    "    dataset = []\n",
    "    for msg in readData(path):\n",
    "        row = {\"message\":msg,\"class\":classification}\n",
    "        dataset.append(row)\n",
    "    return pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame({\"message\":[],\"class\":[]})\n",
    "dataset = dataset.append(df('emails/ham/','ham'))\n",
    "dataset = dataset.append(df('emails/spam/','spam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\n\\n    Date:        Wed, 21 Aug 2002 10:54:46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\n\\nMartin A posted:\\n\\nTassos Papadopoulos, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\n\\nMan Threatens Explosion In Moscow \\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\n\\nKlez: The Virus That Won't Die\\n\\n \\n\\nAlr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\n\\n&gt;  in adding cream to spaghetti carbonara,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                            message\n",
       "0   ham  \\n\\n    Date:        Wed, 21 Aug 2002 10:54:46...\n",
       "1   ham  \\n\\nMartin A posted:\\n\\nTassos Papadopoulos, t...\n",
       "2   ham  \\n\\nMan Threatens Explosion In Moscow \\n\\n\\n\\n...\n",
       "3   ham  \\n\\nKlez: The Virus That Won't Die\\n\\n \\n\\nAlr...\n",
       "4   ham  \\n\\n>  in adding cream to spaghetti carbonara,..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,1].values\n",
    "y = dataset.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ham', 'spam'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n    Date:        Wed, 21 Aug 2002 10:54:46 -0500\\n\\n    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\\n\\n    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\\n\\n\\n\\n\\n\\n  | I can\\'t reproduce this error.\\n\\n\\n\\nFor me it is very repeatable... (like every time, without fail).\\n\\n\\n\\nThis is the debug log of the pick happening ...\\n\\n\\n\\n18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\\n\\n18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\\n\\n18:19:04 Ftoc_PickMsgs {{1 hit}}\\n\\n18:19:04 Marking 1 hits\\n\\n18:19:04 tkerror: syntax error in expression \"int ...\\n\\n\\n\\nNote, if I run the pick command by hand ...\\n\\n\\n\\ndelta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\\n\\n1 hit\\n\\n\\n\\nThat\\'s where the \"1 hit\" comes from (obviously).  The version of nmh I\\'m\\n\\nusing is ...\\n\\n\\n\\ndelta$ pick -version\\n\\npick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\\n\\n\\n\\nAnd the relevant part of my .mh_profile ...\\n\\n\\n\\ndelta$ mhparam pick\\n\\n-seq sel -list\\n\\n\\n\\n\\n\\nSince the pick command works, the sequence (actually, both of them, the\\n\\none that\\'s explicit on the command line, from the search popup, and the\\n\\none that comes from .mh_profile) do get created.\\n\\n\\n\\nkre\\n\\n\\n\\nps: this is still using the version of the code form a day ago, I haven\\'t\\n\\nbeen able to reach the cvs repository today (local routing issue I think).\\n\\n\\n\\n\\n\\n\\n\\n_______________________________________________\\n\\nExmh-workers mailing list\\n\\nExmh-workers@redhat.com\\n\\nhttps://listman.redhat.com/mailman/listinfo/exmh-workers\\n\\n\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in range(len(X)):\n",
    "    tokens.append(word_tokenize(X[i].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords.extend([',','.',\"'\",':','_','}','{',')',']','>','<','+','...','|','$','@','!','-','[','(','/','\\''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for list_1 in tokens:\n",
    "    main_words = []\n",
    "    for word in list_1:\n",
    "        if word not in eng_stopwords:\n",
    "            main_words.append(word)\n",
    "    words.append(main_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'wed', '21', 'aug', '2002', '10:54:46', '-0500', 'chris', 'garrigues', 'cwg-dated-1030377287.06fa6d', 'deepeddy.com', 'message-id', '1029945287.4797.tmda', 'deepeddy.vircio.com', 'ca', \"n't\", 'reproduce', 'error', 'repeatable', 'like', 'every', 'time', 'without', 'fail', 'debug', 'log', 'pick', 'happening', '18:19:03', 'pick_it', 'exec', 'pick', '+inbox', '-list', '-lbrace', '-lbrace', '-subject', 'ftp', '-rbrace', '-rbrace', '4852-4852', '-sequence', 'mercury', '18:19:03', 'exec', 'pick', '+inbox', '-list', '-lbrace', '-lbrace', '-subject', 'ftp', '-rbrace', '-rbrace', '4852-4852', '-sequence', 'mercury', '18:19:04', 'ftoc_pickmsgs', '1', 'hit', '18:19:04', 'marking', '1', 'hits', '18:19:04', 'tkerror', 'syntax', 'error', 'expression', '``', 'int', 'note', 'run', 'pick', 'command', 'hand', 'delta', 'pick', '+inbox', '-list', '-lbrace', '-lbrace', '-subject', 'ftp', '-rbrace', '-rbrace', '4852-4852', '-sequence', 'mercury', '1', 'hit', \"'s\", '``', '1', 'hit', \"''\", 'comes', 'obviously', 'version', 'nmh', \"i'm\", 'using', 'delta', 'pick', '-version', 'pick', '--', 'nmh-1.0.4', 'compiled', 'fuchsia.cs.mu.oz.au', 'sun', 'mar', '17', '14:55:56', 'ict', '2002', 'relevant', 'part', '.mh_profile', 'delta', 'mhparam', 'pick', '-seq', 'sel', '-list', 'since', 'pick', 'command', 'works', 'sequence', 'actually', 'one', \"'s\", 'explicit', 'command', 'line', 'search', 'popup', 'one', 'comes', '.mh_profile', 'get', 'created', 'kre', 'ps', 'still', 'using', 'version', 'code', 'form', 'day', 'ago', 'able', 'reach', 'cvs', 'repository', 'today', 'local', 'routing', 'issue', 'think', '_______________________________________________', 'exmh-workers', 'mailing', 'list', 'exmh-workers', 'redhat.com', 'https', '//listman.redhat.com/mailman/listinfo/exmh-workers']\n"
     ]
    }
   ],
   "source": [
    "print(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words)):\n",
    "    for j in range(len(words[i])):\n",
    "        lemm = wnet.lemmatize(words[i][j], pos='v')\n",
    "        words[i][j] = lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'wed', '21', 'aug', '2002', '10:54:46', '-0500', 'chris', 'garrigues', 'cwg-dated-1030377287.06fa6d', 'deepeddy.com', 'message-id', '1029945287.4797.tmda', 'deepeddy.vircio.com', 'ca', \"n't\", 'reproduce', 'error', 'repeatable', 'like', 'every', 'time', 'without', 'fail', 'debug', 'log', 'pick', 'happen', '18:19:03', 'pick_it', 'exec', 'pick', '+inbox', '-list', '-lbrace', '-lbrace', '-subject', 'ftp', '-rbrace', '-rbrace', '4852-4852', '-sequence', 'mercury', '18:19:03', 'exec', 'pick', '+inbox', '-list', '-lbrace', '-lbrace', '-subject', 'ftp', '-rbrace', '-rbrace', '4852-4852', '-sequence', 'mercury', '18:19:04', 'ftoc_pickmsgs', '1', 'hit', '18:19:04', 'mark', '1', 'hit', '18:19:04', 'tkerror', 'syntax', 'error', 'expression', '``', 'int', 'note', 'run', 'pick', 'command', 'hand', 'delta', 'pick', '+inbox', '-list', '-lbrace', '-lbrace', '-subject', 'ftp', '-rbrace', '-rbrace', '4852-4852', '-sequence', 'mercury', '1', 'hit', \"'s\", '``', '1', 'hit', \"''\", 'come', 'obviously', 'version', 'nmh', \"i'm\", 'use', 'delta', 'pick', '-version', 'pick', '--', 'nmh-1.0.4', 'compile', 'fuchsia.cs.mu.oz.au', 'sun', 'mar', '17', '14:55:56', 'ict', '2002', 'relevant', 'part', '.mh_profile', 'delta', 'mhparam', 'pick', '-seq', 'sel', '-list', 'since', 'pick', 'command', 'work', 'sequence', 'actually', 'one', \"'s\", 'explicit', 'command', 'line', 'search', 'popup', 'one', 'come', '.mh_profile', 'get', 'create', 'kre', 'ps', 'still', 'use', 'version', 'code', 'form', 'day', 'ago', 'able', 'reach', 'cvs', 'repository', 'today', 'local', 'rout', 'issue', 'think', '_______________________________________________', 'exmh-workers', 'mail', 'list', 'exmh-workers', 'redhat.com', 'https', '//listman.redhat.com/mailman/listinfo/exmh-workers']\n"
     ]
    }
   ],
   "source": [
    "print(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words)):\n",
    "    words[i] = ' '.join(words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = tfidf.fit_transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX = vect.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(vect,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2250, 59273), (2250,))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.948"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[633,   0],\n",
       "       [ 39,  78]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(rev):\n",
    "    words = []\n",
    "    tokens = []\n",
    "    text = [rev]\n",
    "    for i in range(len(text)):\n",
    "        tokens.append(word_tokenize(text[i].lower()))\n",
    "    for j in tokens:\n",
    "        main_words = []\n",
    "        for k in j:\n",
    "            if k not in eng_stopwords:\n",
    "                main_words.append(k)\n",
    "        words.append(main_words)\n",
    "    for l in range(len(words)):\n",
    "        for m in range(len(words[l])):\n",
    "            lemm = wnet.lemmatize(words[l][m], pos='v')\n",
    "            words[l][m] = lemm\n",
    "    for n in range(len(words)):\n",
    "        words[n] = ' '.join(words[n])\n",
    "    vect = tfidf.transform(words)\n",
    "    w_arr = vect.toarray()\n",
    "    pred = reg.predict(w_arr)\n",
    "    if pred == [0]:\n",
    "        print(\"ham\")\n",
    "    else:\n",
    "        print(\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = X[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n"
     ]
    }
   ],
   "source": [
    "pred(rev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
